# BenderBox v3.0 Configuration
# =============================
#
# This file configures the BenderBox security analysis platform.
# All paths can be absolute or relative to the project root.
# Environment variables can override any setting (see docs for mappings).

# Local LLM Configuration
# -----------------------
# BenderBox uses local LLMs via llama-cpp-python for offline operation.
llm:
  # Path to analysis model (GGUF format)
  # Recommended: Mistral 7B or similar for general analysis
  analysis_model_path: models/analysis/model.gguf

  # Path to code analysis model (GGUF format)
  # Recommended: CodeLlama or similar for code-specific tasks
  code_model_path: models/code/model.gguf

  # Context window size (tokens)
  # Larger = more context but more memory
  context_length: 4096

  # Number of CPU threads for inference
  # Set to number of physical cores for best performance
  threads: 4

  # Number of layers to offload to GPU
  # 0 = CPU only, -1 = all layers to GPU
  gpu_layers: 0

  # Default generation parameters
  max_tokens: 2048
  temperature: 0.7
  top_p: 0.95

  # Maximum number of models to keep loaded
  # Uses LRU eviction when exceeded
  max_loaded_models: 2

# Storage Configuration
# ---------------------
storage:
  # ChromaDB vector store location
  vector_store_path: data/chromadb

  # SQLite database for reports and metadata
  db_path: data/benderbox.db

  # Knowledge base YAML files location
  knowledge_path: data/knowledge

  # Generated reports storage
  reports_path: data/reports

# Embedding Model Configuration
# -----------------------------
embedding:
  # Model name (HuggingFace) or local path
  # all-MiniLM-L6-v2 is a good balance of size and quality
  model_name_or_path: all-MiniLM-L6-v2

  # Local cache directory for downloaded models
  cache_dir: models/embeddings

  # Offline mode - only use locally cached models
  # Set to false to allow model downloads
  offline_mode: true

  # Embedding cache settings
  enable_cache: true
  cache_size: 10000

# Analysis Configuration
# ----------------------
analysis:
  # Default analysis profile
  # Options: quick, standard, deep, semantic
  default_profile: standard

  # Cache TTL for analysis results (seconds)
  cache_ttl_seconds: 3600

  # Maximum concurrent analyses
  max_concurrent_analyses: 2

  # Enable LLM-powered semantic analysis
  enable_semantic_analysis: true

  # Chunk size for semantic code analysis (characters)
  semantic_chunk_size: 2000

# User Interface Configuration
# ----------------------------
ui:
  # Web UI settings (optional, requires [web] extras)
  web_enabled: false
  web_host: 127.0.0.1  # localhost only for security
  web_port: 8080

  # TUI settings (requires [tui] extras)
  tui_theme: dark  # dark, light, or custom

  # CLI settings
  color_output: true
  progress_indicators: true
